---
title: "Chap. 3&4"
---

# A Formal Learning Model and Learning via Uniform Convergence

### PAC(Probably Approximately correct) learning

A hypothesis class $\mathcal{H}$ is PAC learnable
if there exist a function $m_H : (0; 1)^2 \to \mathcal{N}$ and a learning algorithm with the
following property: For every $\delta, \epsilon \in (0; 1)$, for every distribution $\mathcal{D}$ over $\mathcal{X}$, and
for every labeling function $f : X \to {0; 1}$, if the realizable assumption holds
with respect to $\mathcal{H};\mathcal{D};f$, then when running the learning algorithm on $m \geq 
m_H(\delta; \epsilon)$ i.i.d. examples generated by $\mathcal{D}$ and labeled by $\mathcal{f}$, the algorithm returns
a hypothesis h such that, with probability of at least $1- \delta$ (over the choice of the examples),$ \mathcal{L}_{\mathcal{D},f}(h) \leq \epsilon$

$\epsilon$ - accuracy parameter (how far the output classifier from the optimal) - **Approximately**

$\delta$ - confidence parameter (how likely the classifier is to meet the accuracy requirement) - **Probably**

$m_H : (0; 1)^2 \to \mathcal{N}$ defines sample complexity (how many samples are needed)

### Corollary
Every finite hypothesis class is PAC learnable with following sample complexity:

$m_H(\delta; \epsilon) \leq \frac{\log\frac{\|\mathcal{H}\|}{\delta}}{\epsilon}$


### More general learning rile:

* Realizability assumption does not hold quite often

* Labels may not be fully determined by the features which we measure

$\Rightarrow \mathcal{D}$ should be considered as a probability distribution over $\mathcal{X}\times\mathcal{Y}$.
In other words, $\mathcal{D}$ - joint distribution over domain points and labels.

Let us redefine $\mathcal{L}_{\mathcal{D}}(h)$:

$\mathcal{L}_{\mathcal{D}}(h) = \mathcal{P}_{(x,y)\sim D}[h(x)\neq y] = \mathcal{D}(\{(x,y): h(x) \neq y\})$

And now we want learning algorithm to find a predictor whose error is not much larger than the best possible error of 
a predictor in some hypothesis class, given for bench marking.

#### Agnostic PAC
A hypothesis class $\mathcal{H}$ is agnostic PAC learnable
if there exist a function $m_H : (0; 1)^2 \to \mathcal{N}$ and a learning algorithm with the
following property: For every $\delta, \epsilon \in (0; 1)$, for every distribution $\mathcal{D}$ over $\mathcal{X}\times\mathcal{Y}$, when running learning algorithm on $m \geq 
m_H(\delta; \epsilon)$ i.i.d. examples generated by $\mathcal{D}$, the algorithm returns
a hypothesis $h$ such that, with probability of at least $1- \delta$ (over the choice of the examples),$ \mathcal{L}_{\mathcal{D}}(h) \leq min_{h \in \mathcal{H}} \mathcal{L}_{\mathcal{D}}(h') + \epsilon$

In case of agnostic PAC, if realisability holds Agnostic PAC gives same guarantee as PAC, when realisability does not hold no learner can guarantee an arbitrarily small error, but it is still OK, since learner can still declare a success if its error
is not much larger than the best error achievable by a predictor from the class $\mathcal{H}$.

### Uniform convergence

A training set $\mathcal{S}$ is called $\epsilon$-representative
(w.r.t. domain $\mathcal{Z}$, hypothesis class $\mathcal{H}$, loss function $l$, and distribution $\mathcal{D}$) if 
$\forall h \in \mathcal{H}, \| \mathcal{L}_{\mathcal{S}}(h) -  \mathcal{L}_{\mathcal{D}}(h)\| \leq \epsilon$

Assume that a training set S is $\frac{s}{2}$-representative (w.r.t. domain $\mathcal{Z}$, hypothesis class $\mathcal{H}$, loss function $l$, and distribution $\mathcal{D}$). Then, any output of
$ERM_H(S)$, namely, any $h_S \in argmin_{h \in \mathcal{H}}\mathcal{L}_{\mathcal{S}}(h)$, satisfies
$\mathcal{L}_{\mathcal{D}}(h_S) \leq min_{h \in \mathcal{H}}\mathcal{L}_{\mathcal{D}}(h) + \epsilon$

##### Sketch of proof:

$\mathcal{L}_{\mathcal{D}}(h_s) \leq \mathcal{L}_{\mathcal{S}}(h_s) + \frac{\epsilon}{2} \leq \mathcal{L}_{\mathcal{S}}(h) + \frac{\epsilon}{2} \leq \mathcal{L}_{\mathcal{D}}(h)} + \epsilon$